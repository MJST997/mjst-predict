{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import CamembertTokenizer,RobertaTokenizerFast,AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import CamembertModel,RobertaModel\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import faulthandler\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pynndescent\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"model_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = tf.keras.losses.KLDivergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['company_code',\n",
    " 'dale_chill_formula',\n",
    " 'flesch_score',\n",
    " 'gunnning_fog_score',\n",
    " 'verb_count',\n",
    " 'noun_count',\n",
    " 'adjective_count',\n",
    " 'len_text',\n",
    " 'len_text_words',\n",
    " 'len_mentions',\n",
    " 'score_group',\n",
    " 'prev_likes',\n",
    " 'avg_comp_likes',\n",
    " 'moving_avg_likes',\n",
    " 'Followers',\n",
    " 'hashtag_count',\n",
    " 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the target variable from the rest of the features\n",
    "X = corpus[features]\n",
    "y = corpus['like_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False) #RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "encoded_corpus = tokenizer(text=corpus.lemmatized_text.tolist(),\n",
    "                            add_special_tokens=True,\n",
    "                            padding='max_length',\n",
    "                            truncation='longest_first',\n",
    "                            return_attention_mask=True)\n",
    "input_ids = encoded_corpus['input_ids']\n",
    "attention_mask = encoded_corpus['attention_mask']\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_descriptions(tokenizer, descriptions, max_len):\n",
    "    indices = []\n",
    "    lengths = tokenizer(descriptions, padding=False, \n",
    "                     truncation=False, return_length=True)['length']\n",
    "    for i in range(len(descriptions)):\n",
    "        if lengths[i] <= max_len:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "short_descriptions = filter_long_descriptions(tokenizer, \n",
    "                               corpus.lemmatized_text.tolist(), 300)\n",
    "input_ids = np.array(input_ids)[short_descriptions]\n",
    "attention_mask = np.array(attention_mask)[short_descriptions]\n",
    "labels = y.to_numpy()[short_descriptions]\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([X, pd.DataFrame(input_ids)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "seed = seed_val\n",
    "train_inputs, test_inputs, train_labels, test_labels = \\\n",
    "            train_test_split(X_combined, labels, test_size=test_size, \n",
    "                             random_state=seed)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_mask, \n",
    "                                        labels, test_size=test_size, \n",
    "                                        random_state=seed)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_scaler = StandardScaler()\n",
    "like_scaler.fit(train_labels.reshape(-1, 1))\n",
    "train_labels = like_scaler.transform(train_labels.reshape(-1, 1))\n",
    "test_labels = like_scaler.transform(test_labels.reshape(-1, 1))\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulthandler.enable()\n",
    "batch_size = 256\n",
    "def create_dataloaders(inputs, masks, labels, batch_size):\n",
    "    input_tensor = torch.tensor(inputs)\n",
    "    mask_tensor = torch.tensor(masks)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(input_tensor, mask_tensor, \n",
    "                            labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "    return dataloader\n",
    "train_dataloader = create_dataloaders(train_inputs, train_masks, \n",
    "                                      train_labels, batch_size)\n",
    "test_dataloader = create_dataloaders(test_inputs, test_masks, \n",
    "                                     test_labels, batch_size)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTtweetreg(nn.Module):\n",
    "    \n",
    "    def __init__(self, drop_rate=0.2, freeze_bertweet=False):\n",
    "        \n",
    "        super(BERTtweetreg, self).__init__()\n",
    "        D_in, D_out = 768, 1\n",
    "        \n",
    "        self.bertweet = \\\n",
    "                   AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(D_in, D_out))\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        \n",
    "        outputs = self.bertweet(input_ids, attention_masks)\n",
    "        class_label_output = outputs[1]\n",
    "        outputs = self.regressor(class_label_output)\n",
    "        return outputs\n",
    "model = BERTtweetreg(drop_rate=0.2)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=5e-5,\n",
    "                  eps=1e-8)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,       \n",
    "                 num_warmup_steps=0, num_training_steps=total_steps)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, loss_function, epochs,       \n",
    "          train_dataloader, device, clip_value=2):\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        print(\"-----\")\n",
    "        best_loss = 1e10\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader): \n",
    "            print(step)  \n",
    "            batch_inputs, batch_masks, batch_labels = \\\n",
    "                               tuple(b.to(device) for b in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(batch_inputs, batch_masks)           \n",
    "            loss = loss_function(outputs.squeeze(), \n",
    "                             batch_labels.squeeze())\n",
    "            loss.backward()\n",
    "            clip_grad_norm(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "  \n",
    "    return model\n",
    "model = train(model, optimizer, scheduler, loss_function, epochs, \n",
    "              train_dataloader, device, clip_value=2)\n",
    "print(\"SUCCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_function, test_dataloader, device):\n",
    "    model.eval()\n",
    "    test_loss, test_r2 = [], []\n",
    "    for batch in test_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = \\\n",
    "                                 tuple(b.to(device) for b in batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_inputs, batch_masks)\n",
    "        loss = loss_function(outputs, batch_labels)\n",
    "        test_loss.append(loss.item())\n",
    "        r2 = r2_score(outputs, batch_labels)\n",
    "        test_r2.append(r2.item())\n",
    "    return test_loss, test_r2\n",
    "def r2_score(outputs, labels):\n",
    "    labels_mean = torch.mean(labels)\n",
    "    ss_tot = torch.sum((labels - labels_mean) ** 2)\n",
    "    ss_res = torch.sum((labels - outputs) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "def predict(model, dataloader, device):\n",
    "  model.eval()\n",
    "  output = []\n",
    "  for batch in dataloader:\n",
    "      batch_inputs, batch_masks, _ = \\\n",
    "                                tuple(b.to(device) for b in batch)\n",
    "      with torch.no_grad():\n",
    "          output += model(batch_inputs, \n",
    "                          batch_masks).view(1,-1).tolist()[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_labels.to_numpy()\n",
    "test_labels = like_scaler.transform(test_labels.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_bert = r2_score(torch.FloatTensor(test_labels),torch.FloatTensor(y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
